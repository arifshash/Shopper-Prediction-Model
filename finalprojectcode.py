# -*- coding: utf-8 -*-
"""FinalProjectCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v_GlHfYqR-7_0lMkf2plHuAtzdcWQypq
"""

import pandas as pd 
import matplotlib.pyplot as plt
import numpy as np
import pylab as pl
import seaborn as sns
from pandas.plotting import scatter_matrix
from matplotlib import cm
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
newsdata = pd.read_table('/content/sample_data/online_news_popularity.csv', sep='\s*,\s*') #the sep part removes the white spaces so that the columsn are organizzed
print(newsdata.columns.tolist()) #List all columns

#Method 3
sharescategory = pd.cut(newsdata.shares,bins=[0,3395,843300],labels=['1','2']) #create classes for data based on mean and max 
newsdata.insert(5,'Sharescat',sharescategory) #create new category with the classes 1 or 2 1 being below and 2 being above

newsdata.head()

from pandas.plotting import scatter_matrix 
from matplotlib import cm
attribute_names = ['n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', 'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity', 'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity'] # removed Non-predictive attributes and target attributes
x = newsdata[attribute_names] #Comparitive attributes without url timedelta and shares
y= newsdata['Sharescat'] #Target attribute is sharescat as it is converted from conitious variable to categorical

from sklearn.model_selection import train_test_split #splitting the data into train and test
x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=0)
from sklearn.preprocessing import MinMaxScaler #Normailzing the data 
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

print(x_train.shape)
print(x_test.shape)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier().fit(x_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))
#Accuracy of Decision Tree classifier on training set: 1.00
#Accuracy of Decision Tree classifier on test set: 0.70

pred = clf.predict(x_test)#Run-time: 36s
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(x_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(x_test, y_test)))

#Accuracy of K-NN classifier on training set: 0.83
#Accuracy of K-NN classifier on test set: 0.78

pred = knn.predict(x_test)#Run-time: 36s
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(x_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(x_test, y_test)))
#Accuracy of SVM classifier on training set: 0.80
#Accuracy of SVM classifier on test set: 0.80

pred = svm.predict(x_test)#Run-time: 15
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

"""THIS TRIAL HAD A BETTER RESULT B/C CATEGORIES WAS ASSIGNED TO EACH SHARE BASED OFF OF MAX AND MEAN SHARES (TWO CATEGOREIS) 1 below mean and 1 above. HOWEVER THIS RESULTED IN LOW PRECISION, RECALL, F1 but a higher accuracy. NEXT TRIAL EDIT THE ACTAULL DATA by removing the non predictive attributes completely out instead of just listing them as the comparitive?????

Method 2
"""

#Method 2
newsdata = pd.read_csv('/content/sample_data/online_news_popularity.csv')
attribute_names=news[news.drop(['url',' timedelta',' shares'], axis=1).columns.values.tolist()]
target=news[' shares']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(attribute_names, target, random_state=0)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

x_train = pd.DataFrame(x_train, columns = attribute_names.columns)
x_test = pd.DataFrame(x_test, columns = attribute_names.columns)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif, f_regression, mutual_info_regression, mutual_info_classif

selector = SelectKBest(f_classif, k=5)
selector.fit(x_train, y_train)

cols = selector.get_support(indices=True)
x_train = x_train.iloc[:,cols]
x_test = x_test.iloc[:,cols]

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(x_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(X_test, y_test)))

#Run-time 10s
#Accuracy of K-NN classifier on training set: 0.00
#Accuracy of K-NN classifier on test set: 0.00

pred = knn.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier()
clf.fit(x_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))
#Accuracy of Decision Tree classifier on training set: 0.99
#Accuracy of Decision Tree classifier on test set: 0.02

pred = clf.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train, y_train)
print("Accuracy of SVM classifier on training set: {:.2f}".format(svm.score(x_train, y_train)))
print("Accuracy of SVM classifier on test set: {:.2f}".format(svm.score(x_test, y_test)))

pred = svm.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

#Method 1
from pandas.plotting import scatter_matrix 
from matplotlib import cm
attribute_names = ['n_tokens_title', 'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', 'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words', 'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity', 'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity'] # removed Non-predictive attributes and target attributes
x = newsdata[attribute_names] #Comparitive attributes (non-predictve attributes removed)
y= newsdata['shares'] #Target attribute is shares


from sklearn.model_selection import train_test_split #splitting the data into train and test
x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=0)
from sklearn.preprocessing import MinMaxScaler #Normailzing the data 
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier().fit(x_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))
#Accuracy of Decision Tree classifier on training set: 1.00
#Accuracy of Decision Tree classifier on test set: 0.02

pred = clf.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))


from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(x_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(x_test, y_test)))
#Accuracy of K-NN classifier on training set: 0.23
#Accuracy of K-NN classifier on test set: 0.01

pred = knn.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(x_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(x_test, y_test)))

pred = svm.predict(x_test)
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))

"""DATA SET 2"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pylab as pl
import seaborn as sns
from pandas.plotting import scatter_matrix
from matplotlib import cm
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
shopsdata = pd.read_csv('/content/sample_data/online_shoppers_intention.csv')
print(shopsdata.columns.tolist()) #List all columns

shopsdata.describe() #Shows the statistics of each category/column

names = shopsdata[shopsdata.columns.values.tolist()]
correlations = shopsdata.corr(method='pearson')
fig = plt.figure(figsize=(20,20))
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,18,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names, rotation = 'vertical')
ax.set_yticklabels(names)
plt.savefig('shop_corr') #Identified correlraltion between attributes and how strong they were

for col in ['Month','VisitorType']:
    shopsdata[col] = shopsdata[col].astype('category')

cat_columns = shopsdata.select_dtypes(['category']).columns
cat_columns

shopsdata[cat_columns] = shopsdata[cat_columns].apply(lambda x: x.cat.codes) #Convert categorical to numerical data for month and visitortype
shopsdata['Weekend'] = shopsdata['Weekend'].astype(int)
shopsdata['Revenue'] = shopsdata['Revenue'].astype(int)

"""How i figured this out: https://stackoverflow.com/questions/32011359/convert-categorical-data-in-pandas-dataframe"""

shopsdata.describe()

attribute_names = ['Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend'] # removed target attributes
x = shopsdata[attribute_names] #Comparitive attributes without revenue
y= shopsdata['Revenue'] #Target attribute is revenue

from sklearn.model_selection import train_test_split #splitting the data into train and test
x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=0)
from sklearn.preprocessing import MinMaxScaler #Normailzing the data 
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier().fit(x_train, y_train)
print('Accuracy of Decision Tree classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('Accuracy of Decision Tree classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))
#Accuracy of Decision Tree classifier on training set: 1.00
#Accuracy of Decision Tree classifier on test set: 0.86

pred = clf.predict(x_test)#Run-time: 0s
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred)) #Run-time: 0s

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train) #Run-time: 2s
print('Accuracy of K-NN classifier on training set: {:.2f}'
     .format(knn.score(x_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.2f}'
     .format(knn.score(x_test, y_test)))
#Accuracy of K-NN classifier on training set: 0.89
#Accuracy of K-NN classifier on test set: 0.84

pred = knn.predict(x_test)#Run-time: 0s
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))#Run-time: 0s

from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(x_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(x_test, y_test))) #Run-time: 3s
#Accuracy of SVM classifier on training set: 0.89
#Accuracy of SVM classifier on test set: 0.86

pred = svm.predict(x_test)#Run-time: 0s
print(confusion_matrix(y_test, pred))

print(classification_report(y_test, pred))#Run-time: 0s